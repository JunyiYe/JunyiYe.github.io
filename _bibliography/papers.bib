---
---

@article{rahman2024blind,
  abbr={Preprint},
  bibtex_show={true},
  title={From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical Integrity on Faulty Mathematical Problems},
  author={Rahman*, Muntasir and Ye*, Junyi and Yao, Wei and Yin, Wenpeng and Wang, Guiling},
  journal={arXiv preprint arXiv:2410.18921},
  year={2024},
  abstract={Consider the math problem: "Lily received 3 cookies from her best friend yesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies. How many cookies does Lily have now?'' Many LLMs solve this by calculating "1'' using the equation "3 - 5 + 3." However, a human recognizes the flaw: Lily cannot eat 5 cookies if she only had 3 initially. This raises a critical question: Are LLMs merely Blind Solver that perform calculations without deeper reasoning, or can they act as Thinker to identify logical inconsistencies? To investigate, we introduce FaultyMath, a benchmark of diverse faulty math problems spanning multiple categories (e.g., algebra, geometry), difficulty levels, and origins of faultiness (e.g., common sense violations, ambiguity, contradictions). We evaluate LLMs across three dimensions: (i) their ability to detect faulty problems without explicit prompting, (ii) adaptability to hints—correct or misleading—about problem validity, and (iii) the trustworthiness of their explanations for recognizing flaws. Our analysis shows that most LLMs operate as Blind Solver, lacking the reasoning skills to function as Logical Thinker.},
  pdf={FaultyMathProblem_paper.pdf},
  code={https://github.com/JunyiYe/FaultyMathProblem},
  preview={FaultyMathProblem_logo.png}
}

@article{ye2024beyond,
  abbr={Preprint},
  bibtex_show={true},
  title={Beyond End-to-End VLMs: Leveraging Intermediate Text Representations for Superior Flowchart Understanding},
  author={Ye, Junyi and Dash, Ankan and Yin, Wenpeng and Wang, Guiling},
  journal={arXiv preprint arXiv:2412.16420},
  year={2024},
  abstract={Flowcharts are typically presented as images, driving the trend of using vision-language models (VLMs) for end-to-end flowchart understanding. However, two key challenges arise: (i) Limited controllability—users have minimal influence over the downstream task, as they can only modify input images, while the training of VLMs is often out of reach for most researchers. (ii) Lack of explainability—it is difficult to trace VLM errors to specific causes, such as failures in visual encoding or reasoning. We propose TextFlow, addressing aforementioned issues with two stages: (i) Vision Textualizer—which generates textual representations from flowchart images; and (ii) Textual Reasoner—which performs question-answering based on the text representations. TextFlow offers three key advantages: (i) users can select the type of text representations (e.g., Graphviz, Mermaid, PlantUML), or further convert them into executable graph object to call tools, enhancing performance and controllability; (ii) it improves explainability by helping to attribute errors more clearly to visual or textual processing components; and (iii) it promotes the modularization of the solution, such as allowing advanced LLMs to be used in the reasoner stage when VLMs underperform in end-to-end fashion. Experiments on the FlowVQA and FlowLearn benchmarks demonstrate TextFlow's state-of-the-art performance as well as its robustness. All code will be publicly released.},
  pdf={TextFlow_paper.pdf},
  code={https://github.com/JunyiYe/TextFlow},
  preview={TextFlow_logo.png}
}

@inproceedings{ye2024assessing,
  abbr={AAAI},
  bibtex_show={true},
  title={Assessing the Creativity of LLMs in Proposing Novel Solutions to Mathematical Problems},
  author={Ye, Junyi and Gu, Jingyi and Zhao, Xinyun and Yin, Wenpeng and Wang, Guiling},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2025},
  abstract={The mathematical capabilities of AI systems are complex and multifaceted. Most existing research has predominantly focused on the correctness of AI-generated solutions to mathematical problems. In this work, we argue that beyond producing correct answers, AI systems should also be capable of, or assist humans in, developing novel solutions to mathematical challenges. This study explores the creative potential of Large Language Models (LLMs) in mathematical reasoning, an aspect that has received limited attention in prior research. We introduce a novel framework and benchmark, CreativeMath, which encompasses problems ranging from middle school curricula to Olympic-level competitions, designed to assess LLMs' ability to propose innovative solutions after some known solutions have been provided. Our experiments demonstrate that, while LLMs perform well on standard mathematical tasks, their capacity for creative problem-solving varies considerably. Notably, the Gemini-1.5-Pro model outperformed other LLMs in generating novel solutions. This research opens a new frontier in evaluating AI creativity, shedding light on both the strengths and limitations of LLMs in fostering mathematical innovation, and setting the stage for future developments in AI-assisted mathematical discovery.},
  pdf={CreativeMath_paper.pdf},
  code={https://github.com/JunyiYe/CreativeMath},
  preview={}
}

@inproceedings{ye2024dataframe,
  abbr={ACML},
  bibtex_show={true},
  title={DataFrame {QA}: A Universal {LLM} Framework on DataFrame Question Answering Without Data Exposure},
  author={Junyi Ye and Mengnan Du and Guiling Wang},
  booktitle={The 16th Asian Conference on Machine Learning (Conference Track)},
  year={2024},
  url={https://openreview.net/forum?id=rDNj0enuhc},
  abstract={This paper introduces DataFrame Question Answering (QA), a novel task that utilizes natural language processing (NLP) models to generate Pandas queries for information retrieval and data analysis on dataframes, emphasizing safe and non-revealing data handling. Specifically, our method, leveraging a large language model (LLM), which solely relies on dataframe column names, not only ensures data privacy but also significantly reduces the context window in the prompt, streamlining information processing and addressing major challenges in LLM-based data analysis. We propose DataFrame QA as a comprehensive framework that includes safe Pandas query generation and code execution. Various LLMs are evaluated on the renowned WikiSQL dataset and our newly developed UCI-DataFrameQA, tailored for complex data analysis queries. Our findings indicate that GPT-4 performs well on both datasets, underscoring its capability in securely retrieving and aggregating dataframe values and conducting sophisticated data analyses. This approach, deployable in a zero-shot manner without prior training or adjustments, proves to be highly adaptable and secure for diverse applications.},
  pdf={dataframeqa_paper.pdf},
  poster={dataframeqa_poster.pdf},
  slides={dataframeqa_slides.pdf},
  code={https://github.com/JunyiYe/dataframe-qa}
}

@inproceedings{10.1145/3677052.3698680,
  abbr={ICAIF},
  bibtex_show={true},
  author = {Gu*, Jingyi and Ye*, Junyi and Uddin, Ajim and Wang, Guiling},
  title = {DySTAGE: Dynamic Graph Representation Learning for Asset Pricing via Spatio-Temporal Attention and Graph Encodings},
  year = {2024},
  isbn = {9798400710810},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3677052.3698680},
  doi = {10.1145/3677052.3698680},
  abstract = {Current GNN-based asset price prediction models often focus on a fixed group of assets and their static relationships within the financial network. However, this approach overlooks the reality that the composition of asset pools and their interrelationships evolves over time, necessitating the development of a flexible framework capable of adapting to this dynamism. Accordingly, we propose DySTAGE, a framework with a universal formulation that transforms asset pricing time series into dynamic graphs, accommodating asset addition, deletion, and changes in correlations. Our framework includes a graph learning model specifically designed for this purpose. In our framework, assets at various historical time steps are structured as a sequence of dynamic graphs, where connections between assets reflect their long-term correlations. DySTAGE effectively captures both topological and temporal patterns. The Topological Module deploys Asset Influence Attention to learn global interrelationships among assets, further enhanced by Asset-wise Importance Encoding, Pair-wise Spatial Encoding, and Edge-wise Correlation Encoding. Meanwhile, the Temporal Module encapsulates node representations across the temporal dimension via the attention mechanism. We validate our approach through extensive experiments using three different real-world stock pricing data, demonstrating that DySTAGE surpasses popular benchmarks in return prediction, and offers profitable investment strategies. The code is publicly available under NJIT FinTech Lab’s GitHub1.},
  booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
  pages = {388–396},
  numpages = {9},
  keywords = {Asset Pricing, Dynamic Graph, Graph Neural Networks, Stock Price Prediction},
  location = {Brooklyn, NY, USA},
  series = {ICAIF '24},
  pdf={DySTAGE_paper.pdf},
  slides={DySTAGE_slides.pdf},
  code={https://github.com/JunyiYe/DySTAGE}
}

@inproceedings{10.1145/3677052.3698681,
  abbr={ICAIF},
  bibtex_show={true},
  author = {Gu*, Jingyi and Ye*, Junyi and Wang, Guiling and Yin, Wenpeng},
  title = {Adaptive and Explainable Margin Trading via Large Language Models on Portfolio Management},
  year = {2024},
  isbn = {9798400710810},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3677052.3698681},
  doi = {10.1145/3677052.3698681},
  abstract = {Recent strategies for portfolio management often lack flexibility to adjust funds between long and short positions throughout trading periods. This prevents adapting portfolios to the market, which mitigates risks and seizes opportunities. To address these gaps, we propose an adaptive and explainable framework that integrates Large Language Models (LLMs) with Reinforcement Learning (RL) for dynamic long-short position adjustment in response to evolving market conditions. This approach leverages the recent advancements in LLMs for processing unstructured data and their capacity for explainable reasoning. The framework includes two stages: an Explainable Market Forecasting/Reasoning Pipeline, and a Position Reallocation stage. The Market Forecasting/Reasoning Pipeline allows various LLMs to learn market trends from diverse external data sources and determine optimal adjustment ratios with a clear reasoning path. The Portfolio Reallocation stage interacts with the sequential trading process from a pre-trained RL model to enhance decision-making and transparency. Our framework is flexible to accommodate various external data sources from microeconomics to macroeconomics data, diverse data types including time series and news text, along with multiple LLMs. Experiments demonstrate that our framework effectively achieves three times the return and doubles the Sharpe ratio compared to benchmarks. All the data and code are publicly available under NJIT FinTech Lab’s GitHub1.},
  booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
  pages = {248–256},
  numpages = {9},
  keywords = {Explainable AI, Large Language Model, Market Trend Forecasting, Portfolio Management, Reinforcement Learning},
  location = {Brooklyn, NY, USA},
  series = {ICAIF '24}
}

@article{dash2024high,
  bibtex_show={true},
  title={High Resolution Solar Image Generation Using Generative Adversarial Networks},
  author={Dash, Ankan and Ye, Junyi and Wang, Guiling and Jin, Huiran},
  journal={Annals of Data Science},
  volume={11},
  number={5},
  pages={1545--1561},
  year={2024},
  url={https://doi.org/10.1007/s40745-022-00436-2},
  doi={10.1007/s40745-022-00436-2},
  abstract={We applied Deep Learning algorithm known as Generative Adversarial Networks (GANs) to perform solar image-to-image translation. That is, from Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager (HMI) line of sight magnetogram images to SDO/Atmospheric Imaging Assembly (AIA) 0304-Å images. The Ultraviolet (UV)/Extreme Ultraviolet observations like the SDO/AIA 0304-Å images were only made available to scientists in the late 1990s even though the magnetic field observations like the SDO/HMI have been available since the 1970s. Therefore, by leveraging Deep Learning algorithms like GANs we can give scientists access to complete datasets for analysis. For generating high resolution solar images, we use the Pix2PixHD and Pix2Pix algorithms. The Pix2PixHD algorithm was specifically designed for high resolution image generation tasks, and the Pix2Pix algorithm is by far the most widely used image to image translation algorithm. For training and testing we used the data for the year 2012, 2013 and 2014. After model training, we evaluated the model on the test data. The results show that our deep learning models are capable of generating high resolution (1024 × 1024 pixels) SDO/AIA0304 images from SDO/HMI line of sight magnetograms. Specifically, the pixel-to-pixel Pearson Correlation Coefficient of the images generated by Pix2PixHD and original images is as high as 0.99. The number is 0.962 if Pix2Pix is used to generate images. The results we get for our Pix2PixHD model is better than the results obtained by previous works done by others to generate SDO/AIA 0304 images. Thus, we can use these models to generate AIA0304 images when the AIA0304 data is not available which can be used for understanding space weather and giving researchers the capability to predict solar events such as Solar Flares and Coronal Mass Ejections. As far as we know, our work is the first attempt to leverage Pix2PixHD algorithm for SDO/HMI to SDO/AIA0304 image-to-image translation.},
  publisher={Springer},
  pdf={solar.pdf},
  preview={solar_logo.jpg}
}

@inproceedings{yao2024establishing,
  bibtex_show={true},
  title={Establishing a Baseline for Evaluating Blockchain-Based Self-Sovereign Identity Systems: A Systematic Approach to Assess Capability, Compatibility and Interoperability},
  author={Yao, Wei and Du, Wenlu and Gu, Jingyi and Ye, Junyi and Deek, Fadi P and Wang, Guiling},
  booktitle={Proceedings of the 2024 6th Blockchain and Internet of Things Conference},
  pages={108--119},
  year={2024}
}

@article{ye2024factor,
  abbr={Preprint},
  bibtex_show={true},
  title={From Factor Models to Deep Learning: Machine Learning in Reshaping Empirical Asset Pricing},
  author={Ye*, Junyi and Goswami*, Bhaskar and Gu*, Jingyi and Uddin, Ajim and Wang, Guiling},
  arXiv={arXiv preprint arXiv:2403.06779},
  year={2024}
}

@article{dash2023review,
  bibtex_show={true},
  title={A review of generative adversarial networks (GANs) and its applications in a wide variety of disciplines: from medical to remote sensing},
  author={Dash, Ankan and Ye, Junyi and Wang, Guiling},
  journal={IEEE Access},
  year={2023},
  publisher={IEEE}
}
